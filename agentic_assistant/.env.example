# Core runtime
MODEL_PATH=/home/ubuntu/models/gemma2b.gguf
LLAMA_MAIN_PATH=/home/ubuntu/llama.cpp/build/bin/llama-cli
HOST=0.0.0.0
PORT=8000

# Pi 5 tuning
INFERENCE_THREADS=4
LLM_CONTEXT_TOKENS=2048
MAX_RESPONSE_TOKENS=256
LLM_TEMPERATURE=0.2

# RAG settings
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
RAG_TOP_K=3
RAG_DATA_DIR=./data/rag
LONG_CONTEXT_THRESHOLD_CHARS=1200
CLOUD_TIMEOUT_SECONDS=25
MAX_INPUT_CHARS=8000
EXPOSE_DELIVERY_ERRORS=false

# Hybrid cloud inference (v2)
GROQ_API_KEY=
GROQ_MODEL=llama-3.1-8b-instant

GEMINI_API_KEY=
GEMINI_MODEL=gemini-1.5-flash

KIMI_API_KEY=
KIMI_BASE_URL=https://api.moonshot.ai/v1
KIMI_MODEL=moonshot-v1-8k

# Optional webhook validation
TELEGRAM_BOT_TOKEN=
TELEGRAM_SECRET=
DISCORD_BOT_TOKEN=
DISCORD_BEARER_TOKEN=
WHATSAPP_ACCESS_TOKEN=
WHATSAPP_PHONE_NUMBER_ID=
WHATSAPP_VERIFY_TOKEN=
