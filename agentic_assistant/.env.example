# ---------------------------------------------------------------------------
# Agentic Assistant — environment configuration
# Copy this file to .env and fill in your values.
#
# Windows users: use Windows paths (C:\...) and set BOT_MODE=polling.
# Linux / Raspberry Pi users: use Linux paths (/home/pi/...).
# chmod 600 .env  (Linux/macOS)
# ---------------------------------------------------------------------------

# ---------------------------------------------------------------------------
# Core runtime
# ---------------------------------------------------------------------------

# Windows path examples (uncomment and adjust):
# MODEL_PATH=C:\llama.cpp\models\gemma-2-2b-it-Q4_K_M.gguf
# LLAMA_MAIN_PATH=C:\llama.cpp\build\bin\llama-cli.exe

# Linux / Raspberry Pi path examples:
MODEL_PATH=/home/pi/models/gemma-2-2b-it-Q4_K_M.gguf
LLAMA_MAIN_PATH=/home/pi/llama.cpp/build/bin/llama-cli

HOST=0.0.0.0
PORT=8000

# ---------------------------------------------------------------------------
# Inference tuning
# ---------------------------------------------------------------------------

# Pi 5 tuning — start conservative, increase INFERENCE_THREADS if temps allow
# Windows machines typically have more cores; e.g. set INFERENCE_THREADS=8
INFERENCE_THREADS=4
LLM_CONTEXT_TOKENS=2048
MAX_RESPONSE_TOKENS=256
LLM_TEMPERATURE=0.2
# Subprocess timeout for llama.cpp (seconds).  120s is safe for Pi 5.
# On a fast Windows PC you can reduce this (e.g. 60).
LLAMA_TIMEOUT_SECONDS=120

# ---------------------------------------------------------------------------
# RAG settings
# ---------------------------------------------------------------------------

EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
RAG_TOP_K=3

# Windows example: RAG_DATA_DIR=C:\agentic-assistant\data\rag
RAG_DATA_DIR=./data/rag

# ---------------------------------------------------------------------------
# Safety / limits
# ---------------------------------------------------------------------------

LONG_CONTEXT_THRESHOLD_CHARS=1200
CLOUD_TIMEOUT_SECONDS=25
MAX_INPUT_CHARS=8000
# Set to true only during debugging — never in production
EXPOSE_DELIVERY_ERRORS=false

# ---------------------------------------------------------------------------
# Conversation memory
# ---------------------------------------------------------------------------

# Number of past turns kept per user in SQLite memory
MEMORY_MAX_TURNS=10

# ---------------------------------------------------------------------------
# Routing behaviour
# ---------------------------------------------------------------------------

# USE_LLM_ROUTING=true  → local Gemma decides routing for ambiguous queries
# Set to false for keyword-only routing (faster on low-end hardware)
USE_LLM_ROUTING=true

# Messages shorter than this go straight to local (no routing overhead)
LOCAL_SHORT_THRESHOLD_CHARS=150

# ---------------------------------------------------------------------------
# Bot mode
# ---------------------------------------------------------------------------
# "webhook" — classic mode: Telegram/Discord POST updates to your public URL.
#             Requires a publicly reachable HTTPS endpoint (use ngrok / Cloudflare
#             Tunnel on Windows, or nginx on Linux).
# "polling"  — recommended for Windows: bots actively pull updates.
#              No public URL required.  Works behind NAT/firewalls.
BOT_MODE=polling

# ---------------------------------------------------------------------------
# Hybrid cloud inference (v2)
# Leave blank to disable a cloud route; the agent falls back to local.
# ---------------------------------------------------------------------------

# Groq — fast reasoning.  Keys: https://console.groq.com/keys
GROQ_API_KEY=
GROQ_MODEL=llama-3.1-8b-instant

# Gemini — long context.  Keys: Google AI Studio / Gemini API console
GEMINI_API_KEY=
GEMINI_MODEL=gemini-1.5-flash

# Kimi / Moonshot — planning.  Keys: Moonshot developer console
KIMI_API_KEY=
KIMI_BASE_URL=https://api.moonshot.ai/v1
KIMI_MODEL=moonshot-v1-8k

# ---------------------------------------------------------------------------
# Messaging bot tokens
# ---------------------------------------------------------------------------

# Telegram: BotFather → /newbot → copy token
TELEGRAM_BOT_TOKEN=
# Optional secret sent with each update (webhook mode only)
TELEGRAM_SECRET=

# Discord: discord.com/developers → Bot → copy token
DISCORD_BOT_TOKEN=
# Required for Discord Interactions / Slash Commands (webhook mode only)
DISCORD_PUBLIC_KEY=
# Optional bearer token for legacy webhook auth (webhook mode only)
DISCORD_BEARER_TOKEN=

# ---------------------------------------------------------------------------
# Personality configuration
# ---------------------------------------------------------------------------
# These values shape the assistant's identity, tone, and response style.
# Alternatively, copy personality.yaml.example to personality.yaml and edit it.
# The YAML file takes precedence over these env vars when it exists.

AGENT_NAME=Assistant
AGENT_PERSONALITY=helpful, knowledgeable, and professional
AGENT_RESPONSE_STYLE=concise, clear, and accurate
AGENT_HUMOR=subtle and professional
AGENT_EXPERTISE=general knowledge, technology, and problem-solving

# Path to personality YAML file (optional override)
# Windows example: PERSONALITY_FILE=C:\agentic-assistant\personality.yaml
PERSONALITY_FILE=./personality.yaml
